<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>LLMs for Social Scientists - 5&nbsp; Transformers and Embeddings Fundamentals</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../llms-in-action.html" rel="next">
<link href="../01-fundamentals/01-neural-networks.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="icon" type="image/png" href="assets/images/parrot-icon.png">
<link rel="stylesheet" href="styles.css">
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FQ3CRBNYP0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FQ3CRBNYP0');
</script> 


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals.html">Fundamentals of LLMs</a></li><li class="breadcrumb-item"><a href="../01-fundamentals/02-transformers.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transformers and Embeddings Fundamentals</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">LLMs for Social Scientists</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tapanyemre/LLMs4SocialScientists" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">LLMs for Social Scientists</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-getting-started/01-course-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Course Overview</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-getting-started/02-setup-and-strategies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Technical Setup &amp; Learning Strategies</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fundamentals of LLMs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-fundamentals/01-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-fundamentals/02-transformers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transformers and Embeddings Fundamentals</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../llms-in-action.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs in Action</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-llms-in-action/01-llm-capabilities.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">LLM Capabilities and Limitations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-llms-in-action/02-llm-apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with LLM APIs</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../contextualizing-llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contextualizing LLMs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-contextualizing-llms/01-rag-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">RAG and Context Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-contextualizing-llms/02-llm-showcase.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">LLM Showcase and Interface Design</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">5.1</span> Learning Objectives</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">5.2</span> Introduction</a></li>
  <li><a href="#pre-chapter-learning-2-hours" id="toc-pre-chapter-learning-2-hours" class="nav-link" data-scroll-target="#pre-chapter-learning-2-hours"><span class="header-section-number">5.3</span> Pre-Chapter Learning (2 hours)</a>
  <ul class="collapse">
  <li><a href="#materials" id="toc-materials" class="nav-link" data-scroll-target="#materials"><span class="header-section-number">5.3.1</span> Materials:</a></li>
  </ul></li>
  <li><a href="#session-1-vector-embeddings-deep-dive-30-minutes" id="toc-session-1-vector-embeddings-deep-dive-30-minutes" class="nav-link" data-scroll-target="#session-1-vector-embeddings-deep-dive-30-minutes"><span class="header-section-number">5.4</span> Session 1: Vector Embeddings Deep Dive (30 minutes)</a>
  <ul class="collapse">
  <li><a href="#understanding-how-words-become-numbers" id="toc-understanding-how-words-become-numbers" class="nav-link" data-scroll-target="#understanding-how-words-become-numbers"><span class="header-section-number">5.4.1</span> Understanding How Words Become Numbers</a></li>
  <li><a href="#vector-database-applications" id="toc-vector-database-applications" class="nav-link" data-scroll-target="#vector-database-applications"><span class="header-section-number">5.4.2</span> Vector Database Applications</a></li>
  </ul></li>
  <li><a href="#session-2-self-attention-mechanism-by-hand-40-minutes" id="toc-session-2-self-attention-mechanism-by-hand-40-minutes" class="nav-link" data-scroll-target="#session-2-self-attention-mechanism-by-hand-40-minutes"><span class="header-section-number">5.5</span> Session 2: Self-Attention Mechanism by Hand (40 minutes)</a>
  <ul class="collapse">
  <li><a href="#the-heart-of-transformer-architecture" id="toc-the-heart-of-transformer-architecture" class="nav-link" data-scroll-target="#the-heart-of-transformer-architecture"><span class="header-section-number">5.5.1</span> The Heart of Transformer Architecture</a></li>
  <li><a href="#why-attention-matters" id="toc-why-attention-matters" class="nav-link" data-scroll-target="#why-attention-matters"><span class="header-section-number">5.5.2</span> Why Attention Matters</a></li>
  </ul></li>
  <li><a href="#session-3-complete-transformer-architecture-25-minutes" id="toc-session-3-complete-transformer-architecture-25-minutes" class="nav-link" data-scroll-target="#session-3-complete-transformer-architecture-25-minutes"><span class="header-section-number">5.6</span> Session 3: Complete Transformer Architecture (25 minutes)</a>
  <ul class="collapse">
  <li><a href="#from-attention-to-full-model" id="toc-from-attention-to-full-model" class="nav-link" data-scroll-target="#from-attention-to-full-model"><span class="header-section-number">5.6.1</span> From Attention to Full Model</a></li>
  <li><a href="#transformer-components" id="toc-transformer-components" class="nav-link" data-scroll-target="#transformer-components"><span class="header-section-number">5.6.2</span> Transformer Components</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">5.7</span> Key Concepts</a></li>
  <li><a href="#connection-to-research" id="toc-connection-to-research" class="nav-link" data-scroll-target="#connection-to-research"><span class="header-section-number">5.8</span> Connection to Research</a>
  <ul class="collapse">
  <li><a href="#why-transformers-matter-for-social-science" id="toc-why-transformers-matter-for-social-science" class="nav-link" data-scroll-target="#why-transformers-matter-for-social-science"><span class="header-section-number">5.8.1</span> Why Transformers Matter for Social Science</a></li>
  <li><a href="#research-applications" id="toc-research-applications" class="nav-link" data-scroll-target="#research-applications"><span class="header-section-number">5.8.2</span> Research Applications</a></li>
  </ul></li>
  <li><a href="#advanced-topics-preview" id="toc-advanced-topics-preview" class="nav-link" data-scroll-target="#advanced-topics-preview"><span class="header-section-number">5.9</span> Advanced Topics Preview</a>
  <ul class="collapse">
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">5.9.1</span> Multi-Head Attention</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding"><span class="header-section-number">5.9.2</span> Positional Encoding</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization"><span class="header-section-number">5.9.3</span> Layer Normalization</a></li>
  </ul></li>
  <li><a href="#connection-to-chapter-3" id="toc-connection-to-chapter-3" class="nav-link" data-scroll-target="#connection-to-chapter-3"><span class="header-section-number">5.10</span> Connection to Chapter 3</a></li>
  <li><a href="#navigation" id="toc-navigation" class="nav-link" data-scroll-target="#navigation"><span class="header-section-number">5.11</span> Navigation</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tapanyemre/LLMs4SocialScientists/edit/main/01-fundamentals/02-transformers.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tapanyemre/LLMs4SocialScientists/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals.html">Fundamentals of LLMs</a></li><li class="breadcrumb-item"><a href="../01-fundamentals/02-transformers.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transformers and Embeddings Fundamentals</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transformers and Embeddings Fundamentals</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="learning-objectives" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">5.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you will be able to: - <strong>Embedding Mastery:</strong> Understand how words and concepts are represented as vectors - <strong>Attention Mechanism:</strong> Calculate self-attention by hand and understand its purpose - <strong>Transformer Architecture:</strong> Comprehend the complete transformer model structure - <strong>Query-Key-Value System:</strong> Master the fundamental attention computation framework</p>
</section>
<section id="introduction" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="introduction"><span class="header-section-number">5.2</span> Introduction</h2>
<p>This chapter explores the evolution from neural networks to transformers, the architecture that powers modern Large Language Models. You’ll learn how words become numbers and how attention mechanisms enable models to understand context.</p>
</section>
<section id="pre-chapter-learning-2-hours" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="pre-chapter-learning-2-hours"><span class="header-section-number">5.3</span> Pre-Chapter Learning (2 hours)</h2>
<section id="materials" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="materials"><span class="header-section-number">5.3.1</span> Materials:</h3>
<ol type="1">
<li><strong>Vector Embeddings Foundation</strong> (30 minutes)
<ul>
<li><a href="https://www.youtube.com/watch?v=viZrOnJclY0">Word Embeddings Explained (StatQuest)</a> - 11-minute video on word representations</li>
<li><a href="https://www.ibm.com/think/topics/word-embeddings">What are Word Embeddings? (IBM)</a> - Technical overview (15 min read)</li>
</ul></li>
<li><strong>Transformer Architecture</strong> (60 minutes)
<ul>
<li><a href="https://www.youtube.com/watch?v=iDulhoQ2pro">Attention Is All You Need (Paper Summary)</a> - 30-minute explanation of the original paper</li>
<li>Deep Dive into Transformers by Hand - Detailed mathematical walkthrough (30 min read)</li>
</ul></li>
<li><strong>Attention Mechanisms</strong> (30 minutes)
<ul>
<li>Keys, Queries, Values Explained - Core attention concepts (15 min read)</li>
</ul></li>
</ol>
</section>
</section>
<section id="session-1-vector-embeddings-deep-dive-30-minutes" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="session-1-vector-embeddings-deep-dive-30-minutes"><span class="header-section-number">5.4</span> Session 1: Vector Embeddings Deep Dive (30 minutes)</h2>
<section id="understanding-how-words-become-numbers" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="understanding-how-words-become-numbers"><span class="header-section-number">5.4.1</span> Understanding How Words Become Numbers</h3>
<p><strong>Key Concept</strong>: Vector embeddings transform words into numerical representations that capture meaning and relationships.</p>
<p><strong>Exercise</strong>: Vector Embeddings Practice - Explore how words are represented as vectors - Understand similarity and distance in vector space - Practice with word analogies and relationships</p>
<p><strong>You’ll learn</strong>: How words become numbers that preserve meaning</p>
</section>
<section id="vector-database-applications" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="vector-database-applications"><span class="header-section-number">5.4.2</span> Vector Database Applications</h3>
<ul>
<li><strong>Similarity search</strong>: Finding related concepts</li>
<li><strong>Clustering</strong>: Grouping similar ideas</li>
<li><strong>Recommendation systems</strong>: Suggesting related content</li>
<li><strong>Research applications</strong>: Literature review and synthesis</li>
</ul>
</section>
</section>
<section id="session-2-self-attention-mechanism-by-hand-40-minutes" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="session-2-self-attention-mechanism-by-hand-40-minutes"><span class="header-section-number">5.5</span> Session 2: Self-Attention Mechanism by Hand (40 minutes)</h2>
<section id="the-heart-of-transformer-architecture" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="the-heart-of-transformer-architecture"><span class="header-section-number">5.5.1</span> The Heart of Transformer Architecture</h3>
<p><strong>Key Concept</strong>: Self-attention allows positions to attend to other positions in the sequence, capturing relationships between words.</p>
<p><strong>Exercise</strong>: Self-Attention Calculation - Calculate attention scores step-by-step - Understand query, key, and value matrices - Practice attention computation by hand - Visualize attention patterns</p>
<p><strong>Mathematical Foundation</strong>:</p>
<pre><code>Attention(Q,K,V) = softmax(QK^T/√d_k)V</code></pre>
</section>
<section id="why-attention-matters" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="why-attention-matters"><span class="header-section-number">5.5.2</span> Why Attention Matters</h3>
<ul>
<li><strong>Context understanding</strong>: Words can attend to relevant context</li>
<li><strong>Long-range dependencies</strong>: Capture relationships across long sequences</li>
<li><strong>Interpretability</strong>: Attention weights show what the model focuses on</li>
<li><strong>Flexibility</strong>: Can attend to any position in the sequence</li>
</ul>
</section>
</section>
<section id="session-3-complete-transformer-architecture-25-minutes" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="session-3-complete-transformer-architecture-25-minutes"><span class="header-section-number">5.6</span> Session 3: Complete Transformer Architecture (25 minutes)</h2>
<section id="from-attention-to-full-model" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="from-attention-to-full-model"><span class="header-section-number">5.6.1</span> From Attention to Full Model</h3>
<p><strong>Exercise</strong>: Transformer Architecture Walkthrough - Understand the complete transformer structure - Explore encoder and decoder components - Practice with a complete example - Connect to real-world applications</p>
</section>
<section id="transformer-components" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="transformer-components"><span class="header-section-number">5.6.2</span> Transformer Components</h3>
<ul>
<li><strong>Input Embeddings</strong>: Convert words to vectors</li>
<li><strong>Positional Encoding</strong>: Add position information</li>
<li><strong>Multi-Head Attention</strong>: Multiple attention mechanisms</li>
<li><strong>Feed-Forward Networks</strong>: Process attended information</li>
<li><strong>Layer Normalization</strong>: Stabilize training</li>
<li><strong>Residual Connections</strong>: Help with gradient flow</li>
</ul>
</section>
</section>
<section id="key-concepts" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="key-concepts"><span class="header-section-number">5.7</span> Key Concepts</h2>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
<th>Mathematical Foundation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Vector Embeddings</strong></td>
<td>Dense numerical representations of words/concepts</td>
<td>Continuous vector space mapping</td>
</tr>
<tr class="even">
<td><strong>Self-Attention</strong></td>
<td>Mechanism allowing positions to attend to other positions</td>
<td>Scaled dot-product: Attention(Q,K,V) = softmax(QK^T/√d_k)V</td>
</tr>
<tr class="odd">
<td><strong>Query, Key, Value</strong></td>
<td>Three matrices that enable attention computation</td>
<td>Linear projections: Q=XW_Q, K=XW_K, V=XW_V</td>
</tr>
</tbody>
</table>
</section>
<section id="connection-to-research" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="connection-to-research"><span class="header-section-number">5.8</span> Connection to Research</h2>
<section id="why-transformers-matter-for-social-science" class="level3" data-number="5.8.1">
<h3 data-number="5.8.1" class="anchored" data-anchor-id="why-transformers-matter-for-social-science"><span class="header-section-number">5.8.1</span> Why Transformers Matter for Social Science</h3>
<ul>
<li><strong>Text understanding</strong>: Better comprehension of complex documents</li>
<li><strong>Context awareness</strong>: Models understand relationships between concepts</li>
<li><strong>Scalability</strong>: Can process large amounts of text efficiently</li>
<li><strong>Interpretability</strong>: Attention weights show what models focus on</li>
</ul>
</section>
<section id="research-applications" class="level3" data-number="5.8.2">
<h3 data-number="5.8.2" class="anchored" data-anchor-id="research-applications"><span class="header-section-number">5.8.2</span> Research Applications</h3>
<ul>
<li><strong>Document analysis</strong>: Understanding complex research papers</li>
<li><strong>Survey analysis</strong>: Processing open-ended responses</li>
<li><strong>Literature review</strong>: Synthesizing multiple sources</li>
<li><strong>Content generation</strong>: Creating research summaries</li>
</ul>
</section>
</section>
<section id="advanced-topics-preview" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="advanced-topics-preview"><span class="header-section-number">5.9</span> Advanced Topics Preview</h2>
<section id="multi-head-attention" class="level3" data-number="5.9.1">
<h3 data-number="5.9.1" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">5.9.1</span> Multi-Head Attention</h3>
<ul>
<li>Multiple attention mechanisms running in parallel</li>
<li>Captures different types of relationships</li>
<li>Enables richer representations</li>
</ul>
</section>
<section id="positional-encoding" class="level3" data-number="5.9.2">
<h3 data-number="5.9.2" class="anchored" data-anchor-id="positional-encoding"><span class="header-section-number">5.9.2</span> Positional Encoding</h3>
<ul>
<li>Adds position information to embeddings</li>
<li>Enables understanding of word order</li>
<li>Critical for sequence processing</li>
</ul>
</section>
<section id="layer-normalization" class="level3" data-number="5.9.3">
<h3 data-number="5.9.3" class="anchored" data-anchor-id="layer-normalization"><span class="header-section-number">5.9.3</span> Layer Normalization</h3>
<ul>
<li>Stabilizes training process</li>
<li>Improves convergence</li>
<li>Essential for deep networks</li>
</ul>
</section>
</section>
<section id="connection-to-chapter-3" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="connection-to-chapter-3"><span class="header-section-number">5.10</span> Connection to Chapter 3</h2>
<p>This chapter prepares you for Chapter 3 where you’ll use pre-trained transformer models (BERT, GPT, etc.) through Hugging Face for practical NLP applications. You’ll apply the theoretical understanding gained here to real-world tasks like text classification, NER, and generation.</p>
</section>
<section id="navigation" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="navigation"><span class="header-section-number">5.11</span> Navigation</h2>
<p><strong>Previous:</strong> <a href="../01-fundamentals/01-neural-networks.html">Neural Networks Fundamentals ←</a><br>
<strong>Next:</strong> <a href="../02-llms-in-action/01-llm-capabilities.html">LLM Capabilities →</a></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="tapanyemre/LLMs4SocialScientists" data-repo-id="1029154030" data-category="General" data-category-id="DIC_kwDOLhXqXQc4CbqE" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../01-fundamentals/01-neural-networks.html" class="pagination-link  aria-label=" &lt;span="" networks="" fundamentals&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks Fundamentals</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../llms-in-action.html" class="pagination-link" aria-label="LLMs in Action">
        <span class="nav-page-text">LLMs in Action</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Transformers and Embeddings Fundamentals"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">chapter:</span><span class="co"> true</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>By the end of this chapter, you will be able to:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Embedding Mastery:** Understand how words and concepts are represented as vectors</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention Mechanism:** Calculate self-attention by hand and understand its purpose</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transformer Architecture:** Comprehend the complete transformer model structure</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Query-Key-Value System:** Master the fundamental attention computation framework</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>This chapter explores the evolution from neural networks to transformers, the architecture that powers modern Large Language Models. You'll learn how words become numbers and how attention mechanisms enable models to understand context.</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-Chapter Learning (2 hours)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### Materials:</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Vector Embeddings Foundation** (30 minutes)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="co">[</span><span class="ot">Word Embeddings Explained (StatQuest)</span><span class="co">](https://www.youtube.com/watch?v=viZrOnJclY0)</span> - 11-minute video on word representations</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="co">[</span><span class="ot">What are Word Embeddings? (IBM)</span><span class="co">](https://www.ibm.com/think/topics/word-embeddings)</span> - Technical overview (15 min read)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Transformer Architecture** (60 minutes)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="co">[</span><span class="ot">Attention Is All You Need (Paper Summary)</span><span class="co">](https://www.youtube.com/watch?v=iDulhoQ2pro)</span> - 30-minute explanation of the original paper</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Deep Dive into Transformers by Hand - Detailed mathematical walkthrough (30 min read)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Attention Mechanisms** (30 minutes)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Keys, Queries, Values Explained - Core attention concepts (15 min read)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 1: Vector Embeddings Deep Dive (30 minutes)</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding How Words Become Numbers</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>**Key Concept**: Vector embeddings transform words into numerical representations that capture meaning and relationships.</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>**Exercise**: Vector Embeddings Practice</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explore how words are represented as vectors</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand similarity and distance in vector space</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Practice with word analogies and relationships</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>**You'll learn**: How words become numbers that preserve meaning</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### Vector Database Applications</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Similarity search**: Finding related concepts</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clustering**: Grouping similar ideas</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recommendation systems**: Suggesting related content</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Research applications**: Literature review and synthesis</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 2: Self-Attention Mechanism by Hand (40 minutes)</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Heart of Transformer Architecture</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>**Key Concept**: Self-attention allows positions to attend to other positions in the sequence, capturing relationships between words.</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>**Exercise**: Self-Attention Calculation</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate attention scores step-by-step</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand query, key, and value matrices</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Practice attention computation by hand</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize attention patterns</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>**Mathematical Foundation**:</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="in">Attention(Q,K,V) = softmax(QK^T/√d_k)V</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Attention Matters</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Context understanding**: Words can attend to relevant context</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Long-range dependencies**: Capture relationships across long sequences</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpretability**: Attention weights show what the model focuses on</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flexibility**: Can attend to any position in the sequence</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 3: Complete Transformer Architecture (25 minutes)</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### From Attention to Full Model</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>**Exercise**: Transformer Architecture Walkthrough</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand the complete transformer structure</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explore encoder and decoder components</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Practice with a complete example</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Connect to real-world applications</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformer Components</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input Embeddings**: Convert words to vectors</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Positional Encoding**: Add position information</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-Head Attention**: Multiple attention mechanisms</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feed-Forward Networks**: Process attended information</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer Normalization**: Stabilize training</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual Connections**: Help with gradient flow</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Concepts</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>| Concept | Description | Mathematical Foundation |</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>|---------|-------------|------------------------|</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>| **Vector Embeddings** | Dense numerical representations of words/concepts | Continuous vector space mapping |</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>| **Self-Attention** | Mechanism allowing positions to attend to other positions | Scaled dot-product: Attention(Q,K,V) = softmax(QK^T/√d_k)V |</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>| **Query, Key, Value** | Three matrices that enable attention computation | Linear projections: Q=XW_Q, K=XW_K, V=XW_V |</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection to Research</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Transformers Matter for Social Science</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Text understanding**: Better comprehension of complex documents</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Context awareness**: Models understand relationships between concepts</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scalability**: Can process large amounts of text efficiently</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpretability**: Attention weights show what models focus on</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="fu">### Research Applications</span></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Document analysis**: Understanding complex research papers</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Survey analysis**: Processing open-ended responses</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Literature review**: Synthesizing multiple sources</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Content generation**: Creating research summaries</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Topics Preview</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Head Attention</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple attention mechanisms running in parallel</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures different types of relationships</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables richer representations</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Positional Encoding</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adds position information to embeddings</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables understanding of word order</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Critical for sequence processing</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Layer Normalization</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stabilizes training process</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improves convergence</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Essential for deep networks</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection to Chapter 3</span></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>This chapter prepares you for Chapter 3 where you'll use pre-trained transformer models (BERT, GPT, etc.) through Hugging Face for practical NLP applications. You'll apply the theoretical understanding gained here to real-world tasks like text classification, NER, and generation.</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a><span class="fu">## Navigation</span></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>**Previous:** <span class="co">[</span><span class="ot">Neural Networks Fundamentals ←</span><span class="co">](01-neural-networks.qmd)</span>  </span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>**Next:** <span class="co">[</span><span class="ot">LLM Capabilities →</span><span class="co">](../02-llms-in-action/01-llm-capabilities.qmd)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tapanyemre/LLMs4SocialScientists/edit/main/01-fundamentals/02-transformers.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tapanyemre/LLMs4SocialScientists/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>
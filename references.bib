@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2023survey,
  title={A survey of large language models},
  author={Zhang, Wayne and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{liu2023survey,
  title={A survey on large language model based autonomous agents},
  author={Liu, Zhiheng and Zhao, Xu and Wang, Jingwei and Tang, Jie},
  journal={arXiv preprint arXiv:2308.11432},
  year={2023}
}

@article{wang2023survey,
  title={A survey of large language model based autonomous agents: Architectures, capabilities, and challenges},
  author={Wang, Hongru and others},
  journal={arXiv preprint arXiv:2308.11432},
  year={2023}
}

@article{zhang2023comprehensive,
  title={A comprehensive survey on large language model based autonomous agents},
  author={Zhang, Zhiheng and others},
  journal={arXiv preprint arXiv:2308.11432},
  year={2023}
} 